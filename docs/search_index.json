[
["index.html", "Processamento de Linguagem Natural Chapter 1 Mineração de Textos 1.1 Ciclos de mineração de textos 1.2 Definições 1.3 Criando um projeto", " Processamento de Linguagem Natural Valéria Nicéria 2020-03-02 Chapter 1 Mineração de Textos Atualmente, vivemos na era do Big Data, ou seja, estamos gerando dados a todo momento, porém, na maioria das vezes, são dados não estruturados, como notícias, e-mails e documentos de forma geral. Mineração de textos ou do inglês Text Mining, tem como objetivo, encontrar termos relevantes e estabelecer relacionamento entre eles de acordo com a sua frequência e assim extrair informações de grandes volumes de textos. 1.1 Ciclos de mineração de textos Agora, que sabemos que é possível obter informações, de grandes volumes de textos, vejamos como é o processo de obtenção dessas informações: Começar com uma pergunta: Primeiramente, devemos ter uma pergunta que queremos responder, como, por exemplo: O que as pessoas que estão falando sobre data science? Obter os dados: Agora, que temos um questionamento, precisamos conseguir os dados que o responda, sendo assim, irei utilizar como fonte de dados, o que as pessoas estão conversando no Twitter. Limpar: E com os nossos dados em mãos, iremos realizar outra etapa do processo de mineração de dados, que é a limpeza dos nossos dados, removendo caracteres especiais, como acentos, pontuações, colocando todas as palavras em uma só estrutura, como, por exemplo, maiúsculo ou minúsculo e remover todas as palavras de ligação, como as stopwords: a, e, os, de, com, etc. Que serão irrelevantes para a pergunta que queremos responder. Analisar: Com os nossos dados prontos, iremos realizar uma das partes mais divertidas, que é analisar os nossos dados, onde iremos aplicar diversas técnicas e verificar se com o dados que possuímos, poderemos responder à pergunta que nos motivou a analisar esses dados. Visualizar: Nessa etapa, poderemos visualizar o resultado da nossa análise e assim gerar diversas opções de gráficos, como, por exemplo, nuvem de palavras. Extrair conhecimento: E após, seguirmos, todas as etapas, chegamos a última, e se tudo estiver ocorrido bem, durante o processo de análise, teremos transformado os nossos dados em informação e agregando ao nosso entendimento prévio sobre o assunto, como resultado, gerado um conhecimento novo, sobre o fato que estávamos analisando. 1.2 Definições Antes, de continuarmos, vamos conhecer alguns conceitos: Corpus: Conjuntos de textos. Stopwords: Como comentado anteriormente, são palavras consideradas irrelevantes, para o resultado da mineração e existem stopwords para todos idiomas. 1.3 Criando um projeto Chegou o momento mais divertido onde criaremos um projeto de text mining, e para isso, utilizaremos a linguagem de programação R e os seguintes pacotes: ‘rtweet’ para conectarmos a api do Twitter. ‘tm’ para realizarmos a mineração dos textos. ‘wordcloud’ para criarmos nuvens de palavras. ‘tydeverse’ para facilitar a manipulação dos nossos dados. Primeiramente, vamos instalar os pacotes que serão necessários durante o projeto: # Instalando os pacotes install.packages(&quot;rtweet&quot;) install.packages(&quot;tm&quot;) install.packages(&quot;wordcloud&quot;) install.packages(&quot;tidyverse&quot;) E com os pacotes instalados, devemos carregar os mesmos e assim poderemos utilizar as funções desses pacotes. # Carregando os pacotes library(tm) library(rtweet) library(wordcloud) library(tidyverse) Precisaremos de dados e vamos coletar esses dados utilizando a API do Twitter, usando a função de busca ‘search_tweets()’, passaremos a # que iremos buscar, o número de tweets, onde o número máximo é 18 mil, informaremos que não queremos os retweets e a linguagem dos tweets deverá ser em inglês. # Buscando os tweets com a #datascience datascience_tweet &lt;- search_tweets( &quot;#datascience&quot;, n = 18000, include_rts = FALSE, lang = &quot;en&quot; ) E essas são as primeiras linhas da busca: Visualizando a frequência de tweets utilizando #datascience, no intervalo de 1 hora: # Gerando um gráfico com a frequencia dos tweets no intervalo de 1 hora datascience_tweet %&gt;% ts_plot(&quot;1 hours&quot;) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = &quot;bold&quot;)) + ggplot2::labs( x = NULL, y = NULL, title = &quot;Frequência de #datascience no Twitter&quot;, subtitle = &quot;Tweets no intervalo de 1 hora&quot;, caption = &quot;\\nSource: Dados coletados do Twitter&#39;s REST API via rtweet&quot; ) Vamos começar a mineração dos textos e para isso iremos pegar a (coluna) text e atribuir a uma variável. # Atribuindo os textos a uma variável datascience_texto &lt;- datascience_tweet$text Tranformando os nossos textos em um corpus, para assim podermos realizar a limpeza utilizando a função tm_map, onde removeremos os caracteres especiais, transformaremos todas as letras para minúsculas, removeremos as pontuações e as stopwords em inglês. # Transformando os textos em um corpus datascience_corpus &lt;- VCorpus(VectorSource(datascience_texto)) # Realizando a limpeza do corpus datascience_corpus &lt;- tm_map( datascience_corpus, content_transformer( function(x) iconv(x, from = &#39;UTF-8&#39;, to = &#39;ASCII//TRANSLIT&#39;) ) ) %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeWords, stopwords(&quot;english&quot;)) Após, realizar a limpeza dos nossos textos, chegou o momento de visualizar o resultado em uma nuvem de palavras e iremos utilizar a função brewer.pal, para gerar as cores em hexadecimal, para assim, colorirmos a nossa nuvem. # Lista de cores em hexadecimal paleta &lt;- brewer.pal(8, &quot;Dark2&quot;) # Criando uma nuvem de palavras, com no máximo 100 palavras # onde tenha se repetido ao menos 2 vezes. wordcloud( datascience_corpus, min.freq = 2, max.words = 100, colors = paleta ) Criando uma matriz de termos (DocumentTermMatrix), removendo os termos menos frequentes e somando os termos para assim verificar os termos mais frequentes. # Criando uma matriz de termos datascience_document &lt;- DocumentTermMatrix(datascience_corpus) # Removendo os termos menos frequentes datascience_doc &lt;- removeSparseTerms(datascience_document, 0.98) # Gerando uma matrix ordenada, com o termos mais frequentes datascience_freq &lt;- datascience_doc %&gt;% as.matrix() %&gt;% colSums() %&gt;% sort(decreasing = T) Gerando um dataframe com os termos mais frequentes e visualizando em um gráfico. # Criando um dataframe com as palavras mais frequentes df_datascience &lt;- data.frame( word = names(datascience_freq), freq = datascience_freq ) # Gerando um gráfico da frequência df_datascience %&gt;% filter(!word %in% c(&quot;datascience&quot;, &quot;via&quot;)) %&gt;% subset(freq &gt; 450) %&gt;% ggplot(aes(x = reorder(word, freq), y = freq)) + geom_bar(stat = &quot;identity&quot;, fill=&#39;#0c6cad&#39;, color=&quot;#075284&quot;) + theme(axis.text.x = element_text(angle = 45, hjus = 1)) + ggtitle(&quot;Termos relacionados a Data Science mais frequentes no Twitter&quot;) + labs(y = &quot;Frequência&quot;, x = &quot;Termos&quot;) + coord_flip() "]
]
