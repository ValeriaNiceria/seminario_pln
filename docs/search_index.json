[
["processamento-de-linguagem-natural.html", "Chapter 2 Processamento de Linguagem Natural 2.1 Código exemplo", " Chapter 2 Processamento de Linguagem Natural Enquanto, mineração de texto, busca extrair informação de grandes volumes de textos, processamento de linguagem natural, procura fazer com que os computadores consigam entender ou produzir linguagem natural, que é a linguagem que nós humanos utilizamos para nos comunicar. Existem varias aplicações de processamento de linguagem natural, como, por exemplo: Tradução: O google translate, ou outros aplicativos de tradução. Análise de sentimentos: Onde é possível identificar, por exemplo, se uma campanha de marketing foi recebida de forma positiva ou não, analisando os comentários das pessoas. Reconhecimento de fala: As assistentes de voz, como a Siri ou o Google Assistente. Sintetização de fala: O GPS que fala se você deve ir para direita ou esquerda. Respostas a perguntas: Chatbots, onde algumas empresas estão utilizando como primeiro contato ao cliente. Esses foram somente, alguns exemplos das possibilidades de utilização do processamento de linguagem natural. 2.1 Código exemplo Para exemplificar uma utilização do PLN, tentaremos responder a seguinte pergunta, “Qual é o sentimento das pessoas em relação a economia?”, agora, que definimos, algo, que tentaremos responder, precisaremos de dados, e utilizaremos os comentários do Twitter, como fonte desses dados. O projeto de exemplificação será construido utilizando a linguagem R e será necessário ter os seguintes pacotes instalados em seu computador: ‘tydeverse’ É um pacote, que possui uma coleção de pacotes inclusos, para ajudar na manipulação dos dados. ‘rtweet’ É um pacote, que permitirá que você se conecte ao Twitter, caso você tenha uma conta, onde você poderá realizar buscas, com no máximo 18 mil tweets. ‘tm’ O pacote tm de “Text Mining” é um pacote utilizado para trabalharmos textos. ‘wordcloud’ É um pacote que nos permite visualizar de forma rápida, as palavras, utilizando como critério de tamanho, a frequência. ‘syuzhet’ É um pacote, que utilizaremos para classificar as sentimentos. # Instalando os pacotes install.packages(&quot;tydeverse&quot;) install.packages(&quot;rtweet&quot;) install.packages(&quot;tm&quot;) install.packages(&quot;wordcloud&quot;) install.packages(&quot;syuzhet&quot;) Vamos carregar os pacotes. # Carregando os pacotes library(tydeverse) library(rtweet) library(tm) library(wordcloud) library(syuzhet) Após carregarmos os pacotes em nosso computador, vamos buscar os textos, utilizando a função search_tweets do pacote rtweet, onde utilizaremos como amostra 2 mil tweets e esses tweets estarão em inglês. # Buscando tweets relacionados a economia economia_tweets &lt;- search_tweets( &quot;#economy&quot;, n = 1500, include_rts = FALSE, lang = &quot;en&quot; ) Visualizando o uso da #economy em um intervalo de 1 hora. # Gerando um gráfico com a frequência dos tweets no intervalo de 1 hora economia_tweets %&gt;% ts_plot(&quot;1 hours&quot;) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = &quot;bold&quot;)) + ggplot2::labs( x = NULL, y = NULL, title = &quot;Frequência de #economy no Twitter&quot;, subtitle = &quot;Tweets no intervalo de 1 hora&quot;, caption = &quot;\\nSource: Dados coletados do Twitter&#39;s REST API via rtweet&quot; ) Agora, iniciaremos o pré-processamento do nosso texto e para simplificar o nosso trabalho, vamos separar a coluna ‘text’ em uma variável. # Separando o texto economia_text &lt;- economia_tweets$text Para fazer a limpeza dos nossos textos podemos utilizar as funções do pacote tm, ou você pode criar as suas próprias funções, como, no exemplo abaixo: # Função para limpeza dos textos limpar_texto &lt;- function(texto) { # Convertendo o texto para minúsculo texto &lt;- tolower(texto) # Removendo o usuário adicionado no comentário texto &lt;- gsub(&quot;@\\\\w+&quot;, &quot;&quot;, texto) # Removendo as pontuações texto &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, texto) # Removendo links texto &lt;- gsub(&quot;http\\\\w+&quot;, &quot;&quot;, texto) # Removendo tabs texto &lt;- gsub(&quot;[ |\\t]{2,}&quot;, &quot;&quot;, texto) # Removendo espaços no início do texto texto &lt;- gsub(&quot;^ &quot;, &quot;&quot;, texto) # Removendo espaços no final do texto texto &lt;- gsub(&quot; $&quot;, &quot;&quot;, texto) return(texto) } Utilizando a função criada para limpar o texto. # Limpando os textos economia_text &lt;- limpar_texto(economia_text) Mas, ainda utilizaremos o pacote tm para remover as stopwords dos textos, então tranformaremos o nosso texto limpo anteriormente em um corpus e posteriormente removeremos as stopwords. # Convertendo os textos em corpus economia_corpus &lt;- VCorpus(VectorSource(economia_text)) # Removendo stopwords economia_corpus &lt;- economia_corpus %&gt;% tm_map(removeWords, stopwords(&quot;english&quot;)) Após, a limpeza, poderemos visualizar os nossos textos em uma nuvem de palavras, para descobrirmos os termos mais frequentes do nosso conjunto de dados. # Lista de cores em hexadecimal paleta &lt;- brewer.pal(8, &quot;Dark2&quot;) wordcloud( economia_corpus, min.freq = 15, max.words = 250, random.order = F, colors = paleta ) Agora, transformaremos o nosso corpus em uma matriz de documentos-termos, para assim criarmos um gráfico de barras com os termos e sua frequência. # Lista de cores em hexadecimal # Transformando o corpus em matriz de documentos-termos economia_doc &lt;- DocumentTermMatrix(economia_corpus) # Removendo os termos menos frequentes economia_doc1 &lt;- removeSparseTerms(economia_doc, 0.97) # Gerando uma matrix ordenada, com o termos mais frequentes economia_freq &lt;- economia_doc1 %&gt;% as.matrix() %&gt;% colSums() %&gt;% sort(decreasing = T) # Criando um dataframe com as palavras mais frequentes df_economia_freq &lt;- data.frame( word = names(economia_freq), freq = economia_freq ) # Gerando um gráfico da frequência df_economia_freq %&gt;% filter(!word %in% c(&quot;economy&quot;)) %&gt;% subset(freq &gt; 50) %&gt;% ggplot(aes(x = reorder(word, freq), y = freq)) + geom_bar(stat = &quot;identity&quot;, fill=&#39;#0c6cad&#39;, color=&quot;#075284&quot;) + theme(axis.text.x = element_text(angle = 45, hjus = 1)) + ggtitle(&quot;Termos relacionados a Economia mais frequentes no Twitter&quot;) + labs(y = &quot;Frequência&quot;, x = &quot;Termos&quot;) + coord_flip() Criando um dendrograma, que é um diagrama de árvore, onde será possível visualizar o agrupamento dos nossos termos. # Dendrograma -&gt; Visualizando os grupos distancia &lt;- dist(t(economia_doc1), method = &quot;euclidian&quot;) dendrograma &lt;- hclust(d = distancia, method = &quot;complete&quot;) plot(dendrograma, habg = -1, main = &quot;Dendrograma Tweets Economia&quot;, xlab = &quot;Distância&quot;, ylab = &quot;Altura&quot;) Agora, realizaremos a análise de sentimentos, dos nossos tweets e para tal análise utilizaremos a função get_nrc_sentiment do pacote syuzhet, onde passaremos como parametro, os termos da nossa matriz de documentos-termos. E após, obtermos as emoções dos nossos termos, faremos o calculo da frequência dos sentimentos que utilizaram a #economy. # Obtendo os emoções economia_sentimento &lt;- get_nrc_sentiment( economia_doc$dimnames$Terms, language = &quot;english&quot; ) # Calculando a frequência dos sentimentos economia_sentimento_freq &lt;- economia_sentimento %&gt;% colSums() %&gt;% sort(decreasing = T) Com a frequência dos nossos sentimentos calculados, poderemos visualizar o resultado, mas antes, iremos traduzir os sentimentos do inglês para o português e tranformar o resultado em um dataframe para posterirmente gerarmos o gráfico. # Criando um dataframe com os sentimentos traduzidos, que será utilizado como de-para. sentimetos_traducao &lt;- data.frame( sentiment = c( &quot;positive&quot;, &quot;negative&quot;, &quot;trust&quot;, &quot;anticipation&quot;, &quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;surprise&quot;, &quot;anger&quot;, &quot;disgust&quot; ), sentimentos = c( &quot;Positivo&quot;, &quot;Negativo&quot;, &quot;Confiança&quot;, &quot;Antecipação&quot;, &quot;Medo&quot;, &quot;Alegria&quot;, &quot;Tristeza&quot;, &quot;Surpresa&quot;, &quot;Raiva&quot;, &quot;Nojo&quot; ) ) # Tranformando os resultados da frequência em um dataframe # e juntando ao dataframe de tradução df_sentimento &lt;- data.frame( sentiment = names(economia_sentimento_freq), freq = economia_sentimento_freq ) %&gt;% left_join(sentimetos_traducao, by = &quot;sentiment&quot;) %&gt;% dplyr::select(-sentiment) %&gt;% arrange(desc(freq)) # Visualizando a frequência dos sentimentos em relação a #economy ggplot(data = df_sentimento, aes(x = reorder(sentimentos, -freq), y = freq)) + geom_bar(aes(fill=sentimentos), stat = &quot;identity&quot;) + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 45, hjus = 1)) + xlab(&quot;Sentimentos&quot;) + ylab(&quot;Frequência&quot;) + ggtitle(titulo) "]
]
